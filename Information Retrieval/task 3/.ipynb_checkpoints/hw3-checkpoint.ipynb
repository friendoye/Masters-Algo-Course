{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class WikiPage(object):\n",
    "    def __init__(self, url=\"\", title=\"\", info=\"\", index=-1):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.info = info\n",
    "        self.index = int(index)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"WikiPage(url = [ %s ], title = %s, info = %s, index = %d)\" % (self.url, self.title, self.info[:100], self.index)\n",
    "\n",
    "class Query(object):\n",
    "    def __init__(self, index = 0, query = \"\"):\n",
    "        self.index = index\n",
    "        self.query = query\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Query(index = %d, query = %s)\" % (self.index, self.query)\n",
    "\n",
    "\n",
    "def parse_data_file(filename = 'scraped_data_utf8.json'):\n",
    "    page_list = []\n",
    "    page_dict = dict()\n",
    "    \n",
    "    with open(filename) as json_wiki_file:\n",
    "        for line in json_wiki_file:\n",
    "            node = json.loads(line.strip())\n",
    "            page = WikiPage(node['url'], node['title'], node['info'], node['index'])\n",
    "            page_list.append(page)\n",
    "            page_dict[page.index] = page\n",
    "    \n",
    "    max_index = max(map(lambda x: x.index, page_list))\n",
    "    for ind in range(max_index):\n",
    "        if ind not in page_dict:\n",
    "            page_dict[ind] = WikiPage(index = ind)\n",
    "            \n",
    "    return page_dict\n",
    "\n",
    "def parse_query_file(filename = 'qid.csv'):\n",
    "    query_list = []\n",
    "\n",
    "    with open(filename) as json_wiki_file:\n",
    "        for line in json_wiki_file:\n",
    "            index, query = line.strip().split(\",\", 1)\n",
    "            query_list.append(\n",
    "                Query(int(index), query)\n",
    "            )\n",
    "    \n",
    "    return query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()             # Simple lemmatizer\n",
    "#stemmer1 = PorterStemmer()                   # Snowball better than Porter\n",
    "stemmer2 = SnowballStemmer('russian') # Snowball better than Porter\n",
    "stop_words = set(stopwords.words('russian')) # Stop words set\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    # Replaces all punctuation symbols with white spaces\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    \n",
    "    tokens = [_.strip().lower() for _ in word_tokenize(text)]\n",
    "    return list(filter(lambda x: x not in string.punctuation, tokens))\n",
    "\n",
    "# Используем лемматизацию вместе со стеммингом, ибо так немножко лучше результаты.\n",
    "# С алгоритмом все просто:\n",
    "# 1) Бьем на токены токенайзером\n",
    "# 2) Удаляем стоп слова\n",
    "# 3) Шлифуем стеммингом\n",
    "def text_to_tokens(text):\n",
    "    lst = tokenize(text)\n",
    "    #lemmaized = lemmatize_all(lst, lemmatizer)#[lemmatizer.lemmatize(_) for _ in lst] \n",
    "    wo_stop_words = filter(lambda word: word not in stop_words, lst)\n",
    "    return [stemmer2.stem(_) for _ in wo_stop_words]#list(wo_stop_words)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Text\n",
    "import itertools\n",
    "import operator\n",
    "from collections import Counter\n",
    "import textwrap\n",
    "\n",
    "from math import log\n",
    "\n",
    "\n",
    "def find_indexes(word, list_of_pages):\n",
    "    pages_amount = len(list_of_pages)\n",
    "    word_indexes = []\n",
    "    for index in range(pages_amount):\n",
    "        if word in list_of_pages[index]:\n",
    "            word_indexes.append(index + 1)\n",
    "    return word_indexes\n",
    "\n",
    "def calculate_frequency(list_of_pages, docs_length):\n",
    "    return [(doc_len, dict(Counter(page).items())) \n",
    "            for (doc_len, page) in zip(docs_length, list_of_pages)]\n",
    "    \n",
    "class CorpusIndex:\n",
    "    \n",
    "    \"\"\"\n",
    "    Build CorpusIndex from list of texts.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus):\n",
    "        corpus = list(corpus)\n",
    "        print(\"Corpus: Ready\")\n",
    "        docs_length = [len(x) for x in corpus]\n",
    "        print(\"Docs length: Ready\")\n",
    "        average_length = sum(docs_length) / len(docs_length)\n",
    "        print(\"Average length: Ready\")\n",
    "        pages_list = list(map(lambda x: text_to_tokens(x), corpus))\n",
    "        print(\"Pages List: Ready\")\n",
    "        unique_lemms = sorted(list(set(itertools.chain(*pages_list))))\n",
    "        print(\"unique_lemms: Ready\")\n",
    "        inverted_index = list(map(lambda lemma: (lemma, find_indexes(lemma, pages_list)), unique_lemms))\n",
    "        print(\"inverted_index: Ready\")\n",
    "        words_frequency = calculate_frequency(pages_list, docs_length)\n",
    "        print(\"words_frequency: Ready\")\n",
    "        return CorpusIndex(unique_lemms, inverted_index, words_frequency, average_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_frequency_token(token):\n",
    "        word, freq = token.split(\"(\")\n",
    "        freq = int(freq[:-1])\n",
    "        return word, freq\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_disk(path):\n",
    "        with open(path, mode=\"r\") as input_file:\n",
    "            # Unique lemmas\n",
    "            input_file.readline()\n",
    "            unique_lemmas_line = input_file.readline().strip()\n",
    "            unique_lemms = unique_lemmas_line.split(\", \")\n",
    "            input_file.readline()\n",
    "            \n",
    "            # Inverted index\n",
    "            input_file.readline()\n",
    "            word_index_line = input_file.readline().strip()\n",
    "            inverted_index = []\n",
    "            while word_index_line != \"\":\n",
    "                word, indexes =  word_index_line.split(\": \")\n",
    "                indexes = [int(x) for x in indexes.split(\" \")]\n",
    "                inverted_index.append((word, indexes))\n",
    "                word_index_line = input_file.readline().strip()\n",
    "            \n",
    "            # Words frequency:\n",
    "            input_file.readline()\n",
    "            frequency_line = input_file.readline()[:-1]\n",
    "            words_frequency = []\n",
    "            while frequency_line != \"\":\n",
    "                amount, freqs =  frequency_line.split(\": \")\n",
    "                amount = int(amount)\n",
    "                if amount != 0:\n",
    "                    freqs = dict(CorpusIndex._parse_frequency_token(freq) for freq in freqs.split(\", \"))\n",
    "                else:\n",
    "                    freqs = dict()\n",
    "                words_frequency.append((amount, freqs))\n",
    "                frequency_line = input_file.readline()[:-1]\n",
    "                \n",
    "            return CorpusIndex(unique_lemms, inverted_index, words_frequency)\n",
    "    \n",
    "    def __init__(self, unique_lemms, inverted_index, words_frequency, average_length):\n",
    "        self.unique_lemms = unique_lemms\n",
    "        # Refactor!\n",
    "        self.inverted_index = dict(inverted_index)\n",
    "        self.words_frequency = words_frequency\n",
    "        self.average_doc_len = average_length\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.unique_lemms == other.unique_lemms \\\n",
    "                and self.inverted_index == other.inverted_index \\\n",
    "                and self.words_frequency == other.words_frequency\n",
    "        \n",
    "    def __str__(self):\n",
    "        return textwrap.dedent(\"\"\"\n",
    "        CorpusIndex(\n",
    "            Unique lemmas: %s\n",
    "            Inverted index: %s\n",
    "            Words frequency: %s            \n",
    "        )\n",
    "        \"\"\" % (self.unique_lemms, self.inverted_index, self.words_frequency))\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, mode=\"w+\") as output_file:\n",
    "            output_file.write(\"Unique lemmas:\\n\")\n",
    "            output_file.write(\", \".join(self.unique_lemms))\n",
    "            output_file.write(\"\\n\\n\")\n",
    "            output_file.write(\"Inverted index:\\n\")\n",
    "            output_file.write(\"\\n\".join(\"%s: %s\" % (word, \" \".join(str(i) for i in indexes)) \\\n",
    "                                        for (word, indexes) in self.inverted_index.items()))\n",
    "            output_file.write(\"\\n\\n\")\n",
    "            output_file.write(\"Words frequency:\\n\")\n",
    "            output_file.write(\"\\n\".join(\"%d: %s\" % (words_amount, \", \".join(\"%s(%d)\" % pair for pair in freq.items())) \\\n",
    "                                        for (words_amount, freq) in self.words_frequency))\n",
    "            output_file.write(\"\\n\")\n",
    "\n",
    "    def find(self, lemma):\n",
    "        result = self.inverted_index.get(lemma)\n",
    "        return result if result != None else []\n",
    "\n",
    "    def lemma_freq(self, lemma, doc_id):\n",
    "        doc_info = self.words_frequency[doc_id - 1]\n",
    "        if doc_info[0] == 0:\n",
    "            return 0\n",
    "        hits = doc_info[1].get(lemma)\n",
    "        if hits == None:\n",
    "            return 0\n",
    "        return hits\n",
    "        \n",
    "    def search_in_index(self, query, rsv_func):\n",
    "        query_tokens = text_to_tokens(query)\n",
    "                \n",
    "        indexes = list(map(self.find, query_tokens))\n",
    "        match_docIds = list(set(itertools.chain(*indexes)))\n",
    "        \n",
    "        rsv_scores = dict(map(lambda docId: (docId, rsv_func(self, docId, query_tokens)), match_docIds))\n",
    "        sorted_rsv_scores = sorted(rsv_scores.items(), key=operator.itemgetter(1))\n",
    "        sorted_rsv_scores.reverse()\n",
    "\n",
    "        return list(map(lambda x: x[0], sorted_rsv_scores))\n",
    "    \n",
    "    def get_avg_index_len(self):\n",
    "        index_len_sum = sum([len(x[1]) for x in self.inverted_index.items()])\n",
    "        return index_len_sum / len(self.inverted_index)\n",
    "    \n",
    "    def get_max_index_len(self):\n",
    "        index_len_max = max([len(x[1]) for x in self.inverted_index.items()])\n",
    "        return index_len_max\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        dict_len = len(self.unique_lemms)\n",
    "        print(\"Length of the dictionary: %s\" % dict_len)\n",
    "        print(\"Average list of word's positions length: %s\" % self.get_avg_index_len())\n",
    "        print(\"Max list of word's positions length: %s\" % self.get_max_index_len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(corpusIndex, lemma):\n",
    "    indexes = corpusIndex.inverted_index.get(lemma)\n",
    "    docs_amount = 0\n",
    "    if indexes != None:\n",
    "        docs_amount = len(indexes)\n",
    "    N = len(corpusIndex.words_frequency)\n",
    "    Nt = docs_amount\n",
    "    return log(1.0 + (N - Nt + 0.5) / (Nt + 0.5))\n",
    "\n",
    "def idf_simple(corpusIndex, lemma):\n",
    "    indexes = corpusIndex.inverted_index.get(lemma)\n",
    "    docs_amount = 0\n",
    "    if indexes != None:\n",
    "        docs_amount = len(indexes)\n",
    "    N = len(corpusIndex.words_frequency)\n",
    "    Nt = docs_amount\n",
    "    return log(N / Nt)\n",
    "\n",
    "def construct_rsv_func(k1, b,\n",
    "                       idf_func = idf, \n",
    "                       norm_rsv = False,\n",
    "                       use_tfq = False, k2 = 100):\n",
    "    def rsv(corpusIndex, doc_id, query_lemmas):\n",
    "        score, idf_sum = 0.0, 0.0\n",
    "        Ld = float(corpusIndex.words_frequency[doc_id - 1][0])\n",
    "        _L_ = corpusIndex.average_doc_len\n",
    "        for lemma in set(query_lemmas):\n",
    "            if norm_rsv:\n",
    "                idf_sum += idf(corpusIndex, lemma)\n",
    "\n",
    "            f_td = corpusIndex.lemma_freq(lemma, doc_id)\n",
    "            f_tq = query_lemmas.count(lemma)\n",
    "            if f_td == 0:\n",
    "                continue\n",
    "\n",
    "            addition = idf_func(corpusIndex, lemma) * f_td * (k1 + 1) / (k1 * ((1 - b) + b * Ld / _L_) + f_td)\n",
    "            if use_tfq:\n",
    "                addition *= (k2 + 1) * f_tq / (k2 + f_tq)\n",
    "            score += addition\n",
    "\n",
    "        if norm_rsv:\n",
    "            score /= idf_sum\n",
    "\n",
    "        return score\n",
    "    \n",
    "    return rsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ranking(corpus, rsv_func=construct_rsv_func(k1 = 1.2, b = 0.75), extra_log_info = \"\", logging=True):\n",
    "    disk_corpus = corpus\n",
    "\n",
    "    with open(\"nnovik_full_data\", mode=\"w+\") as results_f:\n",
    "        pass\n",
    "\n",
    "    query_list = parse_query_file()\n",
    "    #print(\"\\n\\n\".join(str(x) for x in query_list))\n",
    "\n",
    "    search_results = map(lambda q: (q.index, disk_corpus.search_in_index(q.query, rsv_func)[:3]), query_list)\n",
    "    with open(\"nnovik_full_data\", mode=\"w\") as results_f:\n",
    "        index = 0\n",
    "        for (qindex, resutls) in search_results:\n",
    "            results_f.write(\"%d,%s\\n\" % (qindex, \",\".join(map(lambda x: str(x - 1), resutls))))\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: Ready\n",
      "Docs length: Ready\n",
      "Average length: Ready\n",
      "Pages List: Ready\n",
      "unique_lemms: Ready\n",
      "inverted_index: Ready\n",
      "words_frequency: Ready\n"
     ]
    }
   ],
   "source": [
    "data_dict = parse_data_file()\n",
    "rsv = construct_rsv_func(k1 = 1.2, b = 0.75)\n",
    "\n",
    "text_list = []\n",
    "for ind in range(len(data_dict)):\n",
    "    x = data_dict[ind]\n",
    "    text_list.append(\n",
    "        x.title #+ \" \" + x.info\n",
    "    )\n",
    "corpus = CorpusIndex.from_corpus(text_list)\n",
    "#corpus.save(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Ranking for k1 = 1.2, b = 0.75 (headers) ***\n"
     ]
    }
   ],
   "source": [
    "print(\"*** Ranking for k1 = 1.2, b = 0.75 (headers) ***\")\n",
    "test_ranking(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41677777777777769"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_metrics import mapk\n",
    "\n",
    "def getList(filename):\n",
    "    with open(filename) as input:\n",
    "        return [line[:-1].split(',')[1:] for line in input]\n",
    "    \n",
    "mapk(getList('train_data.csv'), getList('nnovik_full_data'),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus.save(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
