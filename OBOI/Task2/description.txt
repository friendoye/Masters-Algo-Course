Функция process_tsv_with_spark считывает тройки чисел из входного файла с помощью SparkContext, затем для каждой тройки составляет пару (вершина, вес). Затем все пары с одинаковыми вершинами заменяет парой, у которой ключом является эта вершина, а значением - сумма всех весов для данной вершины. После этого сортирует полученные пары по ключу и записывает результат в выходной файл.
Использование Spark на языке Python оказалось более удобным по сравнению с Hadoop на Java. Код для Spark был лаконичнее нежели код для Haddop. Также процесс конфигурации контекста Spark оказался более удобным. Конфигурация Job для Hadoop была довольна громоздка.